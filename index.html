<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="About us : Abstract of Selected Publications">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>About us</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/diag-toolkits/AboutUs">View on GitHub</a>

          <h1 id="project_title">About us</h1>
          <h2 id="project_tagline">Abstract of Selected Publications</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/diag-toolkits/AboutUs/zipball/master" style="display: none;">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/diag-toolkits/AboutUs/tarball/master" style="display: none;">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <p><strong>Title:  Using Resource Use Data and System Logs for HPC System Error Propagation and Recovery Diagnosis</strong><br>
<strong>Collaborators</strong>:  The Alan Turing Institute (UK), The University of Warwick (UK), Intel Corporation (USA), Rutgers University (USA), Texas Advanced Computing Center (USA)<br>
<strong>Publication</strong>:  IEEE ISPA 2019.  (Forthcoming)
          <a href=""></a> 
<br><br>
Analyzing failures is important for the reliability of HPC systems and failure diagnosis based only on system logs is incomplete. Resource use data – made available recently – is another potential source of data for failure analysis. Recent work that combines analysis of system logs with resource use data show promising results. In this paper, we describe a new workflow for combining system resource usage and failure logs for diagnosis.
The workflow – called EXERMEST – identifies significant system counters and events then correlates them to failures and recovery. We apply EXERMEST on the Ranger HPC system cluster logdata and show that it improves diagnosis over previous research.
EXERMEST: (i) show that more system counters and errors can be identified only by applying more feature extractors, (ii) identify CPU I/O bottlenecks and Lustre client eviction, (iii) identify network packet drops and Lustre I/O errors, (iv) identify virtual memory and harddisk I/O errors, (v) show that time-bins of different granularities are required for identifying the errors.
EXERMEST is available on the public domain for supporting system administrators in failure diagnosis.
<hr>
        
        <p><strong>Title:  Towards Comprehensive Dependability-Driven Resource Use and Message Log-Analysis for HPC Systems Diagnosis</strong><br>
<strong>Collaborators</strong>:  The Alan Turing Institute (UK), The University of Warwick (UK), Intel Corporation (USA), Rutgers University (USA), University of Texas at Austin (USA)<br>
<strong>Publication</strong>:  Journal of Parallel and Distributed Computing (JPDC), 2019.  
          <a href="https://doi.org/10.1016/j.jpdc.2019.05.013">https://doi.org/10.1016/j.jpdc.2019.05.013</a> 
<br><br>
Failure analysis plays an important role in the reliability of data centers and high-performance computing (HPC) systems. Recent work have shown that both resource use data and failure logs can, separately and together, be used to detect system failure-inducing errors and diagnose system failures; the result of error propagation and (unsuccessful) execution of error recovery mechanisms. For more accurate and detailed failure diagnosis, knowledge of error propagation patterns and unsuccessful error recovery is important. To improve system reliability, knowledge of recovery protocols deployment is important. This paper describes and demonstrates application of a new diagnostics framework (CORRMEXT). CORRMEXT analyzes and reports error propagation patterns and degrees of success and failure of error recovery protocols. The steps in the framework are correlations of resource use metrics and error messages, and identification of the earliest times of change of system behaviour. The framework is illustrated with analyses of resource use data and message logs for three HPC systems operated by the Texas Advanced Computing Center (TACC). The illustrations are focused on groups of resource use counters and groups of errors; they reveal many interesting insights into patterns of: (i) network data and software errors, (ii) Lustre file-system and Linux operating system process errors, and (iii) memory and storage errors. We also confirm that: (i) correlations of resource use and errors can only be identified by applying different correlation algorithms, and (ii) the earliest times of change in system behaviour can only be identified by analyzing both the correlated resource use counters and correlated errors. We believe CORRMEXT is the first tool that have diagnosed error propagation paths and error recovery attempts on three different HPC systems. CORRMEXT will be put on the public domain to support systems administrators in diagnosing HPC system failures, on August 2018.
          
<hr>
        
        <p><strong>Title:  Enabling Dependability-driven Resource Use and Message Log-Analysis for Cluster System Diagnosis</strong><br>
<strong>Collaborators</strong>:  The Alan Turing Institute (UK), The University of Warwick (UK), Intel Corporation (USA), University of Jos (Nigeria), Texas Advanced Computing Center (USA), University of Texas at Austin (USA)<br>
<strong>Publication</strong>:  IEEE HiPC 2017.  <a href="https://doi.org/10.1109/HiPC.2017.00044">https://doi.org/10.1109/HiPC.2017.00044</a> 
<br><br>
Recent work have used both failure logs and resource use data separately (and together) to detect system failure-inducing errors and to diagnose system failures. System failure occurs as a result of error propagation and the (unsuccessful) execution of error recovery mechanisms. Knowledge of error propagation patterns and unsuccessful error recovery is important for more accurate and detailed failure diagnosis, and knowledge of recovery protocols deployment is important for improving system reliability. This paper presents the CORRMEXT framework which carries failure diagnosis another significant step forward by analyzing and reporting error propagation patterns and degrees of success and failure of error recovery protocols. CORRMEXT uses both error messages and resource use data in its analyses. Application of CORRMEXT to data from the Ranger supercomputer have produced new insights. CORRMEXT has: (i) identified correlations between resource use counters that capture recovery attempts after an error, (ii) identified correlations between error events to capture error propagation patterns within the system, (iii) identified error propagation and recovery paths during system execution to explain system behaviour, (iv) showed that the earliest times of change in system behaviour can only be identified by analyzing both the correlated resource use counters and correlated errors. CORRMEXT will be installed on the HPC clusters at the Texas Advanced Computing Center in Autumn 2017.

<hr>

<p><strong>Title:  Online Failure Prediction for HPC Resources using Decentralized Clustering</strong><br>
<strong>Collaborators</strong>:  Rutgers University (USA), University of Texas at Austin (USA), Xerox Research (USA)<br>
<strong>Publication</strong>:  IEEE HiPC 2014.  <a href="http://dx.doi.org/10.1109/HiPC.2014.7116903">http://dx.doi.org/10.1109/HiPC.2014.7116903</a> 
<br><br>
Ensuring high reliability of large-scale clusters is becoming more critical as the size of these machines continues to grow, since this increases the complexity and amount of interactions between different nodes and thus results in a high failure frequency.  For this reason, predicting node failures in order to prevent errors from happening in the first place has become extremely valuable.  A common approach for failure prediction is to analyze traces of system events to find correlations between event types or anomalous event patterns and node failures, and to use the types or patterns identified as failure predictors at run-time.  However, typical centralized solutions for failure prediction in this manner suffer from high transmission and processing overheads at very large scales.  We present a solution to the problem of predicting compute node soft-lockups in large scale clusters by using a decentralized online clustering algorithm (DOC) to detect anomalies in resource usage logs, which have been shown to correlate to particular types of node failures in supercomputer clusters.  We demonstrate the effectiveness of this system by using the monitoring logs from the Ranger supercomputer at Texas Advanced Computing Center.  Experiments shows that this approach can achieve similar accuracy as other related approaches, while maintaining low RAM and bandwidth usage, with a runtime impact to current running applications of less than 2%.</p>

<hr>
        
<p><strong>Title:  Linking Resource Usage Anomalies with System Failures from Cluster Log Data</strong><br>
<strong>Collaborators</strong>:  University of Texas at Austin (USA), University of Warwick (UK), Texas Advanced Computing Center (USA), Xyratex (UK)<br>
<strong>Publication</strong>:  IEEE SRDS 2013.  <a href="http://dx.doi.org/10.1109/SRDS.2013.20">http://dx.doi.org/10.1109/SRDS.2013.20</a> 
<br><br>
Bursts of abnormally high use of resources are thought to be an indirect cause of failures in large cluster systems, but little work has systematically investigated the role of high resource usage on system failures, largely due to the lack of a comprehensive resource monitoring tool which resolves resource use by job and node.  The recently developed TACC_Stats resource use monitor provides the required resource use data.  We present the ANCOR diagnostics system that applies TACC_Stats data to identify resource use anomalies and applies log analysis to link resource use anomalies with system failures.  Application of ANCOR to first identify multiple sources of resource anomalies on the Ranger supercomputer, then correlate them with failures recorded in the message logs and diagnosing the cause of the failures, has identified four new causes of compute node soft lockups.  ANCOR can be adapted to any system that uses a resource use monitor which resolves resource use by job.</p>

<hr>
        
<p><strong>Title:  Diagnosing the Root-Causes of Failures from Cluster Log Files</strong><br>
<strong>Collaborators</strong>:  Institute of High Performance Computing (Singapore), A*Star Computational Resource Center (Singapore), Texas Advanced Computing Center (USA), University of Texas at Austin (USA)<br>
<strong>Publication</strong>:  IEEE HiPC 2010.  <a href="http://dx.doi.org/10.1109/HIPC.2010.5713159">http://dx.doi.org/10.1109/HIPC.2010.5713159</a> 
<br><br>
System event logs are often the primary source of information for diagnosing (and predicting) the causes of failures for cluster systems.  Due to interactions among the system hardware and software components, the system event logs for large cluster systems are comprised of streams of interleaved events, and only a small fraction of the events over a small time span are relevant to the diagnosis of a given failure.  Furthermore, the process of troubleshooting the causes of failures is largely manual and ad-hoc.  In this paper, we present a systematic methodology for reconstructing event order and establishing correlations among events which indicate the root-causes of a given failure from very large syslogs.  We developed a diagnostics tool, FDiag, to extract the log entries as structured message templates and uses statistical correlation analysis to establish probable cause and effect relationships for the fault being analyzed.  We applied FDiag to analyze failures due to breakdowns in interactions between the Lustre file system and its clients on the Ranger supercomputer at the Texas Advanced Computing Center (TACC).  The results are positive.  FDiag is able to identify the dates and the time periods that contain the significant events which eventually led to the occurrence of compute node soft lockups.</p>

      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">About us maintained by <a href="https://github.com/diag-toolkits">diag-toolkits</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
        <p>Contact: edwardchuah@A where A = acm.org</p>
      </footer>
    </div>

    

  </body>
</html>
